# Gestion de fichiers avec AWS S3

Ce script Python permet de gÃ©rer des fichiers dans un bucket AWS S3. Il est organisÃ© en **3 blocs indÃ©pendants** que vous pouvez exÃ©cuter sÃ©parÃ©ment pour apprendre progressivement.

## ğŸ“‹ PrÃ©requis

Avant de commencer, assurez-vous d'avoir :

1. **AWS CLI configurÃ©** avec vos credentials
   
   **Pour la plupart des Ã©tudiants** (credentials par dÃ©faut) :
   ```bash
   aws configure
   ```
   Vous devrez fournir :
   - AWS Access Key ID
   - AWS Secret Access Key
   - RÃ©gion par dÃ©faut (ex: `eu-west-1`)
   
   **Optionnel : Utilisation d'un profil AWS** (pour les formateurs ou utilisateurs avancÃ©s) :
   
   Si vous utilisez plusieurs comptes AWS, vous pouvez crÃ©er des profils :
   ```bash
   aws configure --profile mon-profil
   ```
   
   Vous pourrez ensuite utiliser ce profil avec l'option `--profile` dans les commandes.

2. **Python 3 installÃ©** et les packages nÃ©cessaires
   ```bash
   pip install boto3 pandas pyarrow
   ```
   - `boto3` : SDK AWS pour Python
   - `pandas` : Manipulation de donnÃ©es
   - `pyarrow` : Support du format Parquet

3. **Le fichier de donnÃ©es** `src/ventes.csv` prÃ©sent dans le rÃ©pertoire du script

## ğŸ¯ Structure du code

Le script contient **4 fonctions principales**, correspondant aux blocs du cours :

| Bloc | Fonction | Description |
|------|----------|-------------|
| **Bloc 1** | `create_bucket()` | CrÃ©e un nouveau bucket S3 |
| **Bloc 2** | `upload_file()` | Upload un fichier local vers S3 |
| **Bloc 3** | `list_bucket()` | Liste les objets prÃ©sents dans le bucket |
| **Bloc 4** | `process_pipeline()` | Traite un fichier brut : transforme et archive |

## ğŸš€ Utilisation en ligne de commande

### MÃ©thode recommandÃ©e : exÃ©cuter bloc par bloc

Cette mÃ©thode vous permet de tester chaque Ã©tape individuellement et de comprendre le fonctionnement progressivement.

#### ğŸ“¦ Bloc 1 : CrÃ©er un bucket S3

**Commande de base** (utilise le nom de bucket par dÃ©faut) :
```bash
python main.py create_bucket
```

**Avec un nom de bucket personnalisÃ©** :
```bash
python main.py create_bucket --bucket mon-bucket-unique-12345
```

> ğŸ’¡ **Note** : Le nom du bucket doit Ãªtre **unique globalement** dans AWS. Si le nom existe dÃ©jÃ , vous obtiendrez une erreur. Choisissez un nom unique avec des chiffres ou des lettres.

**Ce qui se passe** :
- Un nouveau bucket S3 est crÃ©Ã© dans votre rÃ©gion AWS configurÃ©e
- Un message de confirmation s'affiche si la crÃ©ation rÃ©ussit

---

#### ğŸ“¤ Bloc 2 : Uploader un fichier vers S3

**Commande de base** (utilise les paramÃ¨tres par dÃ©faut) :
```bash
python main.py upload_file
```

Cette commande utilise par dÃ©faut :
- Bucket : `oc-datalake-8481716`
- Fichier local : `src/ventes.csv`
- Destination S3 : `raw/current/ventes.csv`

**Avec des paramÃ¨tres personnalisÃ©s** :
```bash
python main.py upload_file \
  --bucket mon-bucket \
  --file src/ventes.csv \
  --s3-key raw/current/ventes.csv
```

**Avec un profil AWS** :
```bash
python main.py upload_file --profile mon-profil
```

**Explication des paramÃ¨tres** :
- `--bucket` : Nom du bucket S3 (doit exister)
- `--file` : Chemin local du fichier Ã  uploader
- `--s3-key` : Chemin de destination dans le bucket (structure de dossiers)

> âš ï¸ **Important** : Le bucket doit exister avant d'y uploader des fichiers. ExÃ©cutez d'abord le Bloc 1 !

---

#### ğŸ“‹ Bloc 3 : Lister les objets du bucket

**Commande de base** :
```bash
python main.py list_bucket
```

Cette commande liste tous les objets dans le dossier `raw/` du bucket par dÃ©faut.

**Avec des paramÃ¨tres personnalisÃ©s** :
```bash
python main.py list_bucket \
  --bucket mon-bucket \
  --prefix raw/
```

**Avec un profil AWS** :
```bash
python main.py list_bucket --profile mon-profil
```

**Explication des paramÃ¨tres** :
- `--bucket` : Nom du bucket S3 Ã  lister
- `--prefix` : Filtre les objets commenÃ§ant par ce prÃ©fixe (comme un dossier)

**Exemple de sortie** :
```
 - raw/current/ventes.csv
```

---

#### ğŸ”„ Bloc 4 : Pipeline de traitement des donnÃ©es

Le pipeline de traitement illustre le principe d'un Data Lake avec deux zones :
- **raw/** : oÃ¹ arrivent les fichiers bruts (CSV)
- **processed/** : oÃ¹ sont stockÃ©s les fichiers transformÃ©s (Parquet)

**Commande de base** :
```bash
python main.py process_pipeline
```

Cette commande utilise par dÃ©faut :
- Fichier source : `raw/current/ventes.csv`
- Fichier transformÃ© : `processed/ventes.parquet`

**Avec des paramÃ¨tres personnalisÃ©s** :
```bash
python main.py process_pipeline \
  --bucket mon-bucket \
  --raw-key raw/current/ventes.csv \
  --processed-key processed/ventes.parquet
```

**Avec un profil AWS** :
```bash
python main.py process_pipeline --profile mon-profil
```

**Ce que fait le pipeline** :

Le pipeline effectue automatiquement les 6 Ã©tapes suivantes :

1. **ğŸ“¥ TÃ©lÃ©chargement** : TÃ©lÃ©charge le fichier CSV depuis `raw/current/`
2. **ğŸ“Š Lecture et validation** : Lit le fichier avec pandas et affiche un aperÃ§u
3. **ğŸ”§ Transformation** : Supprime les lignes avec valeurs manquantes (NaN)
4. **ğŸ’¾ Sauvegarde en Parquet** : Convertit et upload le fichier transformÃ© dans `processed/`
5. **ğŸ“¦ Archivage** : Copie le fichier brut dans `raw/archived/` avec un timestamp
6. **ğŸ—‘ï¸ Nettoyage** : Supprime le fichier de `raw/current/` pour garder cette zone propre

**Exemple de sortie** :
```
ğŸ”„ DÃ©marrage du pipeline de traitement...
   Fichier source: raw/current/ventes.csv

ğŸ“¥ Ã‰tape 1 : TÃ©lÃ©chargement du fichier brut...
   âœ… Fichier tÃ©lÃ©chargÃ©: ventes.csv

ğŸ“Š Ã‰tape 2 : Lecture et validation du fichier...
   âœ… Fichier lu avec succÃ¨s
   ğŸ“ˆ Dimensions: 100 lignes, 5 colonnes

ğŸ”§ Ã‰tape 3 : Transformation des donnÃ©es...
   âœ… Transformation terminÃ©e
   ğŸ—‘ï¸  3 ligne(s) avec valeurs manquantes supprimÃ©e(s)
   ğŸ“ˆ Dimensions finales: 97 lignes, 5 colonnes

ğŸ’¾ Ã‰tape 4 : Sauvegarde en format Parquet...
   âœ… Fichier transformÃ© dÃ©posÃ© dans: processed/ventes.parquet

ğŸ“¦ Ã‰tape 5 : Archivage du fichier brut...
   âœ… Fichier archivÃ© dans: raw/archived/ventes_20241215_143022.csv

ğŸ—‘ï¸  Ã‰tape 6 : Suppression du fichier de raw/current/...
   âœ… Fichier supprimÃ© de: raw/current/ventes.csv

âœ… Pipeline terminÃ© avec succÃ¨s!
   ğŸ“ Fichier transformÃ©: processed/ventes.parquet
   ğŸ“ Fichier archivÃ©: raw/archived/ventes_20241215_143022.csv
```

**Explication des paramÃ¨tres** :
- `--bucket` : Nom du bucket S3
- `--raw-key` : ClÃ© S3 du fichier brut Ã  traiter (dans `raw/current/`)
- `--processed-key` : ClÃ© S3 de destination pour le fichier transformÃ© (dans `processed/`)

**Organisation du Data Lake aprÃ¨s traitement** :

```
bucket/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ current/          (vide aprÃ¨s traitement)
â”‚   â””â”€â”€ archived/
â”‚       â””â”€â”€ ventes_20241215_143022.csv  (historique)
â””â”€â”€ processed/
    â””â”€â”€ ventes.parquet    (donnÃ©es transformÃ©es prÃªtes pour l'analyse)
```

> ğŸ’¡ **Pourquoi ce pattern ?**
> - `raw/current/` reste propre avec uniquement les fichiers en attente
> - `raw/archived/` conserve un historique complet pour la traÃ§abilitÃ©
> - `processed/` contient les donnÃ©es optimisÃ©es (Parquet) pour l'analyse
> - Format Parquet : plus efficace que CSV pour l'analyse (compression, colonnes)

---

#### ğŸ”„ ExÃ©cuter tous les blocs en une fois

Si vous voulez exÃ©cuter les 3 blocs dans l'ordre :
```bash
python main.py all
```

**Avec un profil AWS** :
```bash
python main.py all --profile mon-profil
```

Cette commande exÃ©cute automatiquement :
1. CrÃ©ation du bucket
2. Upload du fichier
3. Liste des objets

---

### ğŸ“– Obtenir de l'aide

Pour voir toutes les options disponibles :
```bash
python main.py --help
```

Pour voir l'aide d'une action spÃ©cifique :
```bash
python main.py create_bucket --help
```

---

## ğŸ’» Utilisation dans un script Python

Vous pouvez Ã©galement importer les fonctions dans vos propres scripts Python ou notebooks Jupyter :

```python
from main import create_bucket, upload_file, list_bucket, process_pipeline
import os

# Bloc 1 : CrÃ©ation du bucket
bucket_name = "oc-datalake-8481716"
create_bucket(bucket_name)

# Bloc 2 : Upload du fichier
local_file = os.path.join("src", "ventes.csv")
upload_file(bucket_name, local_file, "raw/current/ventes.csv")

# Bloc 3 : Liste des objets
list_bucket(bucket_name, prefix="raw/")

# Bloc 4 : Pipeline de traitement
process_pipeline(bucket_name, "raw/current/ventes.csv")
```

---

## ğŸ“š DÃ©tails des fonctions

### `create_bucket(bucket_name)`

CrÃ©e un nouveau bucket S3.

**ParamÃ¨tres :**
- `bucket_name` (str) : Nom du bucket (doit Ãªtre unique globalement dans AWS)

**Retour :** Aucun (affiche un message de confirmation)

**Exemple :**
```python
create_bucket("mon-bucket-unique-12345")
```

---

### `upload_file(bucket_name, local_file_path, s3_key)`

Upload un fichier local vers le bucket S3.

**ParamÃ¨tres :**
- `bucket_name` (str) : Nom du bucket S3 (doit exister)
- `local_file_path` (str) : Chemin local du fichier Ã  uploader
- `s3_key` (str) : Chemin de destination dans le bucket S3 (structure de dossiers)

**Retour :** Aucun (affiche un message de confirmation)

**Exemple :**
```python
upload_file("mon-bucket", "src/ventes.csv", "raw/current/ventes.csv")
```

---

### `list_bucket(bucket_name, prefix="")`

Liste les objets dans un bucket S3.

**ParamÃ¨tres :**
- `bucket_name` (str) : Nom du bucket S3
- `prefix` (str, optionnel) : PrÃ©fixe pour filtrer les objets (par dÃ©faut : chaÃ®ne vide)

**Retour :** Aucun (affiche la liste des objets)

**Exemple :**
```python
list_bucket("mon-bucket", prefix="raw/")
```

---

### `process_pipeline(bucket_name, raw_key, processed_key=None)`

Traite un fichier brut du Data Lake : tÃ©lÃ©charge, transforme, sauvegarde en Parquet et archive.

**ParamÃ¨tres :**
- `bucket_name` (str) : Nom du bucket S3
- `raw_key` (str) : ClÃ© S3 du fichier brut dans `raw/current/` (ex: `"raw/current/ventes.csv"`)
- `processed_key` (str, optionnel) : ClÃ© S3 de destination dans `processed/` (par dÃ©faut: `"processed/ventes.parquet"`)

**Retour :** Aucun (affiche les Ã©tapes du pipeline)

**Exemple :**
```python
process_pipeline("mon-bucket", "raw/current/ventes.csv")
```

**Ce que fait la fonction :**
1. TÃ©lÃ©charge le fichier CSV depuis `raw/current/`
2. Lit et valide le contenu avec pandas
3. Transforme les donnÃ©es (suppression des NaN)
4. Sauvegarde en Parquet dans `processed/`
5. Archive le fichier brut dans `raw/archived/` avec timestamp
6. Supprime le fichier de `raw/current/`

---

## âš ï¸ Notes importantes et dÃ©pannage

### Erreurs courantes

1. **"BucketAlreadyExists"** ou **"BucketAlreadyOwnedByYou"**
   - Le nom du bucket existe dÃ©jÃ . Choisissez un nom unique.
   - Solution : Utilisez `--bucket` avec un nom diffÃ©rent

2. **"NoSuchBucket"**
   - Vous essayez d'uploader ou lister un bucket qui n'existe pas.
   - Solution : CrÃ©ez d'abord le bucket avec `create_bucket`

3. **"FileNotFoundError"**
   - Le fichier local spÃ©cifiÃ© n'existe pas.
   - Solution : VÃ©rifiez le chemin du fichier avec `--file`

4. **"AccessDenied"** ou **"InvalidAccessKeyId"**
   - ProblÃ¨me de credentials AWS.
   - Solution : VÃ©rifiez votre configuration avec `aws configure`
   - Si vous utilisez un profil : vÃ©rifiez que le profil existe avec `aws configure list-profiles`

5. **"ProfileNotFound"**
   - Le profil AWS spÃ©cifiÃ© n'existe pas.
   - Solution : VÃ©rifiez le nom du profil ou crÃ©ez-le avec `aws configure --profile nom-profil`

6. **"NoSuchKey"** (lors du pipeline)
   - Le fichier brut n'existe pas dans `raw/current/`.
   - Solution : Assurez-vous d'avoir uploadÃ© le fichier avec `upload_file` avant de lancer le pipeline

7. **"ModuleNotFoundError: No module named 'pandas'"** ou **"No module named 'pyarrow'"**
   - Les dÃ©pendances pandas ou pyarrow ne sont pas installÃ©es.
   - Solution : Installez-les avec `pip install pandas pyarrow`

### Utilisation des profils AWS (optionnel)

Si vous travaillez avec plusieurs comptes AWS ou environnements, vous pouvez utiliser des profils :

**CrÃ©er un profil** :
```bash
aws configure --profile mon-profil
```

**Lister vos profils** :
```bash
aws configure list-profiles
```

**Utiliser un profil dans le script** :
```bash
python main.py create_bucket --profile mon-profil
```

> ğŸ’¡ **Note pour les Ã©tudiants** : Si vous n'utilisez qu'un seul compte AWS, vous n'avez pas besoin d'utiliser `--profile`. Le script utilisera automatiquement vos credentials par dÃ©faut configurÃ©s avec `aws configure`.

### Bonnes pratiques

- âœ… Le nom du bucket doit Ãªtre **unique globalement** dans AWS
- âœ… Utilisez des noms de bucket en minuscules, sans espaces
- âœ… Assurez-vous d'avoir les **permissions nÃ©cessaires** sur votre compte AWS
- âœ… Le bucket doit Ãªtre **crÃ©Ã© avant** d'y uploader des fichiers
- âœ… Utilisez des prÃ©fixes (comme `raw/current/`) pour organiser vos fichiers
- âœ… Utilisez `--profile` si vous travaillez avec plusieurs comptes AWS

### Ordre d'exÃ©cution recommandÃ©

Pour suivre le cours, exÃ©cutez les blocs dans cet ordre :

1. **Bloc 1** : `python main.py create_bucket`
2. **Bloc 2** : `python main.py upload_file`
3. **Bloc 3** : `python main.py list_bucket`
4. **Bloc 4** : `python main.py process_pipeline` (traite le fichier uploadÃ© dans le Bloc 2)

---

## ğŸ” VÃ©rification

AprÃ¨s avoir exÃ©cutÃ© les blocs, vous pouvez vÃ©rifier dans la console AWS :
1. Connectez-vous Ã  [AWS Console](https://console.aws.amazon.com/)
2. Allez dans le service **S3**
3. Ouvrez votre bucket

**AprÃ¨s les Blocs 1-3** :
- Vous devriez voir le fichier `raw/current/ventes.csv`

**AprÃ¨s le Bloc 4 (pipeline)** :
- `raw/current/` devrait Ãªtre vide
- `raw/archived/` devrait contenir le fichier archivÃ© avec timestamp
- `processed/` devrait contenir le fichier `ventes.parquet`
