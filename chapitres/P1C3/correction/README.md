# Pipeline de traitement IoT - GreenFarm Data Lake

Ce script Python permet de traiter des donnÃ©es IoT issues de capteurs installÃ©s dans les champs de GreenFarm. Il gÃ¨re un pipeline complet de traitement de donnÃ©es JSON vers Parquet dans un Data Lake AWS S3.

## ğŸ“‹ PrÃ©requis

Avant de commencer, assurez-vous d'avoir :

1. **AWS CLI configurÃ©** avec vos credentials
   ```bash
   aws configure
   ```
   Vous devrez fournir :
   - AWS Access Key ID
   - AWS Secret Access Key
   - RÃ©gion par dÃ©faut (ex: `eu-west-1`)

2. **Python 3 installÃ©** et les packages nÃ©cessaires
   ```bash
   pip install boto3 pandas pyarrow
   ```
   - `boto3` : SDK AWS pour Python
   - `pandas` : Manipulation de donnÃ©es
   - `pyarrow` : Support du format Parquet

3. **Le fichier de donnÃ©es** `src/iot.json` prÃ©sent dans le rÃ©pertoire du script

## ğŸ¯ Structure du code

Le script contient **4 fonctions principales** :

| Fonction | Description |
|----------|-------------|
| `create_bucket()` | CrÃ©e un nouveau bucket S3 |
| `upload_file()` | Upload un fichier local vers S3 |
| `process_pipeline()` | Traite un fichier JSON IoT : transforme et archive |
| `list_bucket()` | Liste les objets prÃ©sents dans le bucket |

## ğŸš€ Utilisation en ligne de commande

### MÃ©thode recommandÃ©e : exÃ©cuter bloc par bloc

#### ğŸ“¦ Ã‰tape 1 : CrÃ©er un bucket S3

**âš ï¸ Important** : Le nom du bucket doit Ãªtre unique globalement dans AWS. Remplacez `datalex` par vos initiales ou un identifiant personnel.

**Commande de base** :
```bash
python main.py create_bucket
```

**Avec un nom de bucket personnalisÃ©** :
```bash
python main.py create_bucket --bucket greenfarm-datalake-demo-vos-initiales
```

**Avec un profil AWS** :
```bash
python main.py create_bucket --profile mon-profil
```

**Ce qui se passe** :
- Un nouveau bucket S3 est crÃ©Ã© dans votre rÃ©gion AWS configurÃ©e
- Un message de confirmation s'affiche si la crÃ©ation rÃ©ussit

---

#### ğŸ“¤ Ã‰tape 2 : Uploader le fichier JSON vers S3

**Commande de base** :
```bash
python main.py upload_file
```

Cette commande utilise par dÃ©faut :
- Bucket : `greenfarm-datalake-demo-datalex`
- Fichier local : `src/iot.json`
- Destination S3 : `raw/current/iot.json`

**Avec des paramÃ¨tres personnalisÃ©s** :
```bash
python main.py upload_file \
  --bucket greenfarm-datalake-demo-vos-initiales \
  --file src/iot.json \
  --s3-key raw/current/iot.json
```

> âš ï¸ **Important** : Le bucket doit exister avant d'y uploader des fichiers. ExÃ©cutez d'abord l'Ã©tape 1 !

---

#### ğŸ”„ Ã‰tape 3 : Pipeline de traitement des donnÃ©es IoT

Le pipeline effectue les transformations suivantes sur les donnÃ©es IoT :
- Conversion des timestamps au bon format
- Renommage des colonnes (temperature â†’ temp_c)
- Tri par device_id et timestamp
- Calcul d'une moyenne glissante sur 3 mesures (temp_c_roll3)

**Commande de base** :
```bash
python main.py process_pipeline
```

Cette commande utilise par dÃ©faut :
- Fichier source : `raw/current/iot.json`
- Fichier transformÃ© : `processed/iot.parquet`

**Avec des paramÃ¨tres personnalisÃ©s** :
```bash
python main.py process_pipeline \
  --bucket greenfarm-datalake-demo-vos-initiales \
  --raw-key raw/current/iot.json \
  --processed-key processed/iot.parquet
```

**Avec un profil AWS** :
```bash
python main.py process_pipeline --profile mon-profil
```

**Ce que fait le pipeline** :

Le pipeline effectue automatiquement les 6 Ã©tapes suivantes :

1. **ğŸ“¥ TÃ©lÃ©chargement** : TÃ©lÃ©charge le fichier JSON depuis `raw/current/`
2. **ğŸ“Š Lecture et validation** : Lit le fichier avec pandas et affiche un aperÃ§u
3. **ğŸ”§ Transformation** :
   - Conversion des timestamps
   - Renommage des colonnes
   - Tri des donnÃ©es
   - Calcul de la moyenne glissante
4. **ğŸ’¾ Sauvegarde en Parquet** : Convertit et upload le fichier transformÃ© dans `processed/`
5. **ğŸ“¦ Archivage** : Copie le fichier brut dans `raw/archived/` avec un timestamp
6. **ğŸ—‘ï¸ Nettoyage** : Supprime le fichier de `raw/current/` pour garder cette zone propre

**Exemple de sortie** :
```
ğŸ”„ DÃ©marrage du pipeline de traitement IoT...
   Fichier source: raw/current/iot.json

Ã‰tape 1 : TÃ©lÃ©chargement du fichier brut...
   âœ… Fichier tÃ©lÃ©chargÃ©: iot.json

Ã‰tape 2 : Lecture et validation du fichier...
   âœ… Fichier lu avec succÃ¨s
   Dimensions: 100 lignes, 4 colonnes

   AperÃ§u des donnÃ©es:
   ...

Ã‰tape 3 : Transformation des donnÃ©es...
   - Conversion des timestamps...
   - Renommage des colonnes...
   - Tri des donnÃ©es par device_id et timestamp...
   - Calcul de la moyenne glissante (rolling 3)...
   âœ… Transformation terminÃ©e
   Dimensions finales: 100 lignes, 5 colonnes

Ã‰tape 4 : Sauvegarde en format Parquet...
   âœ… Fichier transformÃ© dÃ©posÃ© dans: processed/iot.parquet

Ã‰tape 5 : Archivage du fichier brut...
   âœ… Fichier archivÃ© dans: raw/archived/iot_20241215_143022.json

Ã‰tape 6 : Suppression du fichier de raw/current/...
   âœ… Fichier supprimÃ© de: raw/current/iot.json

âœ… Pipeline terminÃ© avec succÃ¨s!
   ğŸ“ Fichier transformÃ©: processed/iot.parquet
   ğŸ“ Fichier archivÃ©: raw/archived/iot_20241215_143022.json
```

---

#### ğŸ“‹ Ã‰tape 4 : Lister les objets du bucket

**Commande de base** :
```bash
python main.py list_bucket
```

Cette commande liste tous les objets dans le dossier `raw/` du bucket par dÃ©faut.

**Avec des paramÃ¨tres personnalisÃ©s** :
```bash
python main.py list_bucket \
  --bucket greenfarm-datalake-demo-vos-initiales \
  --prefix processed/
```

---

#### ğŸ”„ ExÃ©cuter toutes les Ã©tapes en une fois

Si vous voulez exÃ©cuter toutes les Ã©tapes dans l'ordre :
```bash
python main.py all
```

**Avec un profil AWS** :
```bash
python main.py all --profile mon-profil
```

Cette commande exÃ©cute automatiquement :
1. CrÃ©ation du bucket
2. Upload du fichier
3. Pipeline de traitement
4. Liste des objets

---

### ğŸ“– Obtenir de l'aide

Pour voir toutes les options disponibles :
```bash
python main.py --help
```

Pour voir l'aide d'une action spÃ©cifique :
```bash
python main.py create_bucket --help
```

---

## ğŸ’» Utilisation dans un script Python

Vous pouvez Ã©galement importer les fonctions dans vos propres scripts Python ou notebooks Jupyter :

```python
from main import create_bucket, upload_file, process_iot_pipeline, list_bucket
import os

# Ã‰tape 1 : CrÃ©ation du bucket
bucket_name = "greenfarm-datalake-demo-vos-initiales"
create_bucket(bucket_name)

# Ã‰tape 2 : Upload du fichier
local_file = os.path.join("src", "iot.json")
upload_file(bucket_name, local_file, "raw/current/iot.json")

# Ã‰tape 3 : Pipeline de traitement
process_iot_pipeline(bucket_name, "raw/current/iot.json")

# Ã‰tape 4 : Liste des objets
list_bucket(bucket_name, prefix="processed/")
```

---

## ğŸ“š DÃ©tails des transformations

Le pipeline applique les transformations suivantes aux donnÃ©es IoT :

1. **Conversion des timestamps** : `pd.to_datetime()` pour convertir les timestamps en format datetime
2. **Renommage** : `temperature` â†’ `temp_c` pour plus de clartÃ©
3. **Tri** : Par `device_id` puis par `timestamp` pour un ordre logique
4. **Moyenne glissante** : Calcul de `temp_c_roll3` (moyenne sur 3 mesures) par device

Ces transformations prÃ©parent les donnÃ©es pour l'analyse et l'exploitation.

---

## âš ï¸ Notes importantes et dÃ©pannage

### Erreurs courantes

1. **"BucketAlreadyExists"** ou **"BucketAlreadyOwnedByYou"**
   - Le nom du bucket existe dÃ©jÃ . Choisissez un nom unique.
   - Solution : Utilisez `--bucket` avec un nom diffÃ©rent (remplacez 'datalex' par vos initiales)

2. **"NoSuchBucket"**
   - Vous essayez d'uploader ou traiter un bucket qui n'existe pas.
   - Solution : CrÃ©ez d'abord le bucket avec `create_bucket`

3. **"FileNotFoundError"**
   - Le fichier local spÃ©cifiÃ© n'existe pas.
   - Solution : VÃ©rifiez le chemin du fichier avec `--file`

4. **"AccessDenied"** ou **"InvalidAccessKeyId"**
   - ProblÃ¨me de credentials AWS.
   - Solution : VÃ©rifiez votre configuration avec `aws configure`

5. **"ProfileNotFound"**
   - Le profil AWS spÃ©cifiÃ© n'existe pas.
   - Solution : VÃ©rifiez le nom du profil ou crÃ©ez-le avec `aws configure --profile nom-profil`

6. **"NoSuchKey"** (lors du pipeline)
   - Le fichier brut n'existe pas dans `raw/current/`.
   - Solution : Assurez-vous d'avoir uploadÃ© le fichier avec `upload_file` avant de lancer le pipeline

7. **"ModuleNotFoundError: No module named 'pandas'"** ou **"No module named 'pyarrow'"**
   - Les dÃ©pendances pandas ou pyarrow ne sont pas installÃ©es.
   - Solution : Installez-les avec `pip install pandas pyarrow`

### Bonnes pratiques

- âœ… Le nom du bucket doit Ãªtre **unique globalement** dans AWS
- âœ… Utilisez des noms de bucket en minuscules, sans espaces
- âœ… Remplacez `datalex` dans le nom du bucket par vos initiales ou un identifiant personnel
- âœ… Assurez-vous d'avoir les **permissions nÃ©cessaires** sur votre compte AWS
- âœ… Le bucket doit Ãªtre **crÃ©Ã© avant** d'y uploader des fichiers
- âœ… Utilisez des prÃ©fixes (comme `raw/current/`) pour organiser vos fichiers
- âœ… Utilisez `--profile` si vous travaillez avec plusieurs comptes AWS

### Ordre d'exÃ©cution recommandÃ©

Pour suivre l'exercice, exÃ©cutez les Ã©tapes dans cet ordre :

1. **Ã‰tape 1** : `python main.py create_bucket --bucket greenfarm-datalake-demo-vos-initiales`
2. **Ã‰tape 2** : `python main.py upload_file`
3. **Ã‰tape 3** : `python main.py process_pipeline`
4. **Ã‰tape 4** : `python main.py list_bucket`

---

## ğŸ” VÃ©rification

AprÃ¨s avoir exÃ©cutÃ© toutes les Ã©tapes, vous pouvez vÃ©rifier dans la console AWS :
1. Connectez-vous Ã  [AWS Console](https://console.aws.amazon.com/)
2. Allez dans le service **S3**
3. Ouvrez votre bucket

**Structure attendue** :
- `raw/current/` devrait Ãªtre vide
- `raw/archived/` devrait contenir le fichier archivÃ© avec timestamp
- `processed/` devrait contenir le fichier `iot.parquet`

---

## ğŸ“ Contexte de l'exercice

GreenFarm continue sa croissance et souhaite mieux exploiter ses donnÃ©es pour optimiser ses cultures et sa distribution. Ce pipeline traite des donnÃ©es issues de capteurs IoT installÃ©s dans les champs, qui collectent rÃ©guliÃ¨rement des informations comme la tempÃ©rature, l'humiditÃ© du sol ou le taux d'ensoleillement.

Les donnÃ©es sont exportÃ©es chaque jour sous forme de fichier JSON et traitÃ©es par ce pipeline pour Ãªtre transformÃ©es en format Parquet optimisÃ© pour l'analyse.

